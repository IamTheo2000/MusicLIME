{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20e4e85d",
   "metadata": {},
   "source": [
    "## Model Related\n",
    "\n",
    "This notebook contains information on the models trained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6643cb9",
   "metadata": {},
   "source": [
    "#### Model Architecture\n",
    "\n",
    "The models had the following architecture. More information about the training process can be found in the train notebooks. We cannot provide the lyrics or the audios used to train the models. The the state dictionaries of the trained models can be found in this [kaggle dataset](https://kaggle.com/datasets/f46aac43cb37b8053aa15212fbfcc4d6100ef01f59d98b3ccc85e6eeb916e6c4) or through these google drive files for [emotion_lyrics](https://drive.google.com/file/d/1M_APdPNt2lFN2iTbo4dm5lCh_ywHyfei/view?usp=sharing), [genre_lyrics](https://drive.google.com/file/d/1121iKa9a4RFJsqMG1lbv5oaNNd7cXHk2/view?usp=sharing), [emotion_audio](https://drive.google.com/file/d/1zxAmu-c0ajO18LlXslaIUXjHxzuu-LJJ/view?usp=sharing), [genre_audio](https://drive.google.com/file/d/1Z_jFYqIaEd-0hRr44qbQJgfah-VuhpnX/view?usp=sharing), [emotion_multimodal](https://drive.google.com/file/d/1-5NkCMTJsLCim0gq3lV6v0wJyEVkdf6I/view?usp=sharing) and [genre_multimodal](https://drive.google.com/file/d/1-2fuWi4Ym6TkfAE4cHRC0VvHAbogh5h5/view?usp=sharing). The entire training process for the genre tasks is provided in the notebooks: [lyrics](https://www.kaggle.com/theo2000/genre-lyrics), [audio](https://www.kaggle.com/theo2000/genre-audio) and [multimodal](https://colab.research.google.com/drive/1IKhhWebCYLYeYC5bke48VRkJz9AQ6nLg?usp=sharing) and for the emotion task in the following notebooks: [lyrics](https://colab.research.google.com/drive/1SBey52JwQxN5xOSetAsfFcUy9IQMK_fy?usp=sharing), [audio](https://colab.research.google.com/drive/1v2lr4ALc7vrvxJGincM9iR_o8yjnGiGJ?usp=sharing) and [multimodal](https://colab.research.google.com/drive/1_772x4BKEbtwFAIj1x8Y44boc5cqbxys?usp=sharing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fded200f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForAudioClassification, ASTModel, RobertaModel\n",
    "import torch.nn as nn\n",
    "\n",
    "# Language model\n",
    "AutoModelForSequenceClassification.from_pretrained('roberta-large', num_labels=9).to(device)\n",
    "\n",
    "# Audio model\n",
    "model = AutoModelForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", num_labels=9, ignore_mismatched_sizes=True)\n",
    "\n",
    "# The multimodal model consists of these two classes\n",
    "class CombinedClassificationHead(nn.Module):\n",
    "    def __init__(self, audio_feature_size, text_feature_size, num_labels):\n",
    "        super().__init__()\n",
    "        combined_feature_size = audio_feature_size + text_feature_size\n",
    "        self.layer_norm = nn.LayerNorm(combined_feature_size)\n",
    "        self.fc = nn.Linear(combined_feature_size, num_labels)\n",
    "\n",
    "    def forward(self, combined_features):\n",
    "        normalized_features = self.layer_norm(combined_features)\n",
    "        logits = self.fc(normalized_features)\n",
    "        return logits\n",
    "\n",
    "class AudioTextClassificationModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "        self.audio_model = ASTModel.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\")\n",
    "        self.text_model = RobertaModel.from_pretrained('roberta-large')\n",
    "\n",
    "        audio_feature_size = self.audio_model.config.hidden_size\n",
    "        text_feature_size = self.text_model.config.hidden_size\n",
    "\n",
    "        self.classifier = CombinedClassificationHead(audio_feature_size, text_feature_size, num_labels)\n",
    "\n",
    "    def forward(self, input_values, input_ids, attention_mask):\n",
    "        audio_output = self.audio_model(input_values=input_values)\n",
    "        audio_pooled_output = audio_output[1]\n",
    "\n",
    "        text_output = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = text_output[0]\n",
    "        text_pooled_output = sequence_output[:, 0]\n",
    "\n",
    "        combined_features = torch.cat((audio_pooled_output, text_pooled_output), dim=1)\n",
    "        class_logits = self.classifier(combined_features)\n",
    "        return class_logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
