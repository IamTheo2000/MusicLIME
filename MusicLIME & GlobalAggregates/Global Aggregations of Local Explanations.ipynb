{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ff846a",
   "metadata": {},
   "source": [
    "### Global Aggregations of Local Explanations\n",
    "\n",
    "This notebook includes the various methods to generate global aggregates mentioned in the paper. For more information you can access this [paper](https://arxiv.org/abs/1907.03039).\n",
    "\n",
    "To generate global aggregates we followed a two step process. Firstly we generate local explanations for many instances of a dataset, sum the weights of common feautres and calculate the total number each feature appears. Then we calculate the LIME importance, the average importance and the homogeneity wheighted importance. An overview of these processes are in the cells below. We choose to present the multimodal approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81204a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeMusicExplainer(class_names=...)\n",
    "model.eval()\n",
    "\n",
    "def default_dict_of_float():\n",
    "    return defaultdict(float)\n",
    "\n",
    "def default_dict_of_int():\n",
    "    return defaultdict(int)\n",
    "\n",
    "feature_weights = defaultdict(default_dict_of_float)\n",
    "feature_counts = defaultdict(default_dict_of_int)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm.tqdm(test_loader)):\n",
    "        if idx >= 8:\n",
    "            break\n",
    "\n",
    "        specs = batch['spectogram'].to(device)\n",
    "        audios = batch['audio'].cpu().numpy()\n",
    "        lyrics = batch['lyrics']\n",
    "        tokens = batch['ids'].to(device)\n",
    "        masks = batch['mask'].to(device)\n",
    "        \n",
    "        outputs = model(input_values=specs, input_ids=tokens, attention_mask=masks)\n",
    "        softmax_scores = torch.softmax(outputs, dim=1)\n",
    "        preds = torch.argmax(softmax_scores, dim=1)\n",
    "\n",
    "\n",
    "        for indx, (audio, text) in enumerate(zip(audios, lyrics)):\n",
    "            predicted_class = preds[indx].item()\n",
    "            factorization = OpenumixFactorization(audio, temporal_segmentation_params=10, composition_fn=None)\n",
    "            explanation = explainer.explain_instance(factorization, text, predict_fn, num_samples=5000, labels=(predicted_class,))\n",
    "            components, weights = explanation.get_sorted_components(label=predicted_class, negative_components=False)\n",
    "            for i in range(len(weights)):\n",
    "                feature_weights[predicted_class][components[i]] += abs(weights[i])\n",
    "                feature_counts[predicted_class][components[i]] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592bf4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict function for the multimodal apprach. The dataset should take text and waveforms as inputs and give the corresponding token ids, attention masks and spectrograms.\n",
    "def predict_fn(texts, waveforms, batch_size=256):\n",
    "\n",
    "    all_probabilities = []\n",
    "    dataset = MultimodalmDataset(texts, waveforms, feature_extractor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_masks = batch['attention_mask'].to(device)\n",
    "        batch_specs = batch['spectrogram'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_values=batch_specs, input_ids=input_ids, attention_mask=attention_masks)\n",
    "            batch_probabilities = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "        \n",
    "        all_probabilities.extend(batch_probabilities)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return np.array(all_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e8f1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_influences(feature_weights, feature_counts):\n",
    "    Icj_LIME = {c: {j: math.sqrt(weight) for j, weight in features.items()} for c, features in feature_weights.items()}\n",
    "    Icj_AVG = {\n",
    "        c: {j: feature_weights[c][j] / feature_counts[c][j] if feature_counts[c][j] > 0 else 0 for j in feature_weights[c]}\n",
    "        for c in feature_weights\n",
    "    }\n",
    "\n",
    "    total_feature_importance = {}\n",
    "    for c in Icj_LIME:\n",
    "        for j in Icj_LIME[c]:\n",
    "            if j in total_feature_importance:\n",
    "                total_feature_importance[j] += Icj_LIME[c][j]\n",
    "            else:\n",
    "                total_feature_importance[j] = Icj_LIME[c][j]\n",
    "\n",
    "    p_cj = {\n",
    "        c: {j: Icj_LIME[c][j] / total_feature_importance[j] if total_feature_importance[j] > 0 else 0 for j in Icj_LIME[c]}\n",
    "        for c in Icj_LIME\n",
    "    }\n",
    "    H_j = {\n",
    "        j: -sum(p_cj[c].get(j, 0) * math.log(p_cj[c].get(j, 0), 2) if p_cj[c].get(j, 0) > 0 else 0 for c in p_cj)\n",
    "        for j in set(j for c in p_cj for j in p_cj[c])\n",
    "    }\n",
    "    H_min = min(H_j.values())\n",
    "    H_max = max(H_j.values())\n",
    "    \n",
    "    Icj_H = {c: {j: (1 - (H_j[j] - H_min) / (H_max - H_min)) * Icj_LIME[c][j] for j in Icj_LIME[c]} for c in Icj_LIME}\n",
    "    \n",
    "    return Icj_LIME, Icj_AVG, Icj_H"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
